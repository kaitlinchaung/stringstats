import numpy as np
import pandas as pd
from tqdm import tqdm,tqdm_notebook, tqdm_pandas
import os
import glob
import pickle
import argparse
import scipy
import scipy.stats
import statsmodels.api as sm
from pathlib import Path 


# ## Columns
# Many different p-values are generated by NOMAD and output to spectral_pvals.tsv. The corresponding BY-corrected q-values are included in spectral_pvals_corrected.tsv.
# We provide 4 variants of p-values which split off a train dataset, and optimize the p-value objective on this held out dataset:
# **pval_spectral:** train test split, spectral initialization on train, then EM optimizaiton
# **pval_rand_init_EM:** train test split, random initialization, EM optimization
# **pvals_optimized_samplesheet:**  train test split, fixed samplesheet cj, optimized f
# **pvals_optimized_samplesheetSigned:** train test split, samplesheet cj initialization, EM optimization (with cj sign constraint)


# In addition to this, we provide the classical NOMAD p-value, a variant where *f,c* are drawn to be continuous valued, and a variant related to correspondence analysis:
# **nomad_pv:** normal NOMAD p-value, ${numRandNOMAD} random {-1,1} valued c and {0,1} valued f used
# **nomad_cts_pv:** NOMAD p-value, ${numRandNOMAD} random [-1,1] valued c and [0,1] valued f used (uniform)
# **nomad_spectral_corrAnalysis_pv:** train test split, simple correspondence analysis style SVD to compute c and f

# We further provide tighter, asymptotically valid p-values for these quantities:
# **nomad_asymp_pv:** NOMAD asymptotic p-value, ${numRandNOMAD} random {-1,1} valued c and {0,1} valued f used
# **nomad_asymp_cts_pv:** NOMAD asymptotic p-value, ${numRandNOMAD} random [-1,1] valued c and [0,1] valued f used (uniform)
# **nomad_spectral_corrAnalysis_asymp_pv:** train test split, simple correspondence analysis style SVD to compute c and f, asymptotic p-value

# Finally, we also provide 2 generic p-values, which are fundamentally only asymptotically valid:
# **chi2pval:** chi-squared test
# **lrtpval:** likelihood ratio test


### read in samplsheet, and output samplesheetCj dataframe
def parseSamplesheet(samplesheet):
    sheetCj = 0
    success = False
    if samplesheet!='':
        with open(samplesheet,'r') as f:
            cols = f.readline().split(',')
        if len(cols)==1: ### if len(cols) is 1, then only samplesheet name, no ids
            print("Only 1 samplesheet column, using random cjs")
        elif len(cols)>2:
            print("Improperly formatted samplesheet")
        else:

            sheetdf = pd.read_csv(samplesheet,names=['fname','sheetCjs'])
            #### this approach needs to be changed for 10X
            sheetdf['sample'] = (sheetdf.fname
                            .str.rsplit('/',1,expand=True)[1]
                            .str.split('.',1,expand=True)[0])
            sheetdf = sheetdf.drop(columns='fname')
            sheetdf['sheetCjs'] = normalizevec(sheetdf.sheetCjs,-1,1)

            sheetCj = sheetdf.set_index('sample').T
            print('using samplesheet metadata: not fully tested, be warned')
            success=True
    return success, sheetCj

### construct contingency tables, only for anchors in anchLst
def constructCountsDf(args,anchLst):
    df = pd.read_csv(args.infile, delim_whitespace=True, names=['counts','seq','sample'])
    print('done reading')

    ### split seq into anchor and target
    ### for some reason some files have duplicates, so drop those
    df['anchor'] = df.seq.str[:args.kmer_size]
    df['target'] = df.seq.str[args.kmer_size:]
    df = df.drop(columns='seq').drop_duplicates()
    if len(anchLst)>0:
        df = df[df.anchor.isin(anchLst)]

    ### add in informational columns for each anchor for filtering purposes
    ### this is faster than filter command
    df['anch_uniqTargs'] = df.groupby('anchor').target.transform('nunique')
    df['anch_samples']= df.groupby('anchor')['sample'].transform('nunique')
    df['anchSample_cts'] = df.groupby(['anchor','sample']).counts.transform('sum')
    df['anch_cts'] = df.groupby('anchor').counts.transform('sum') ## number of reads per anchor

    df = df[
        (df.anch_cts > args.anchor_count_threshold) &
        (df.anch_uniqTargs > args.anchor_unique_targets_threshold) &
        (df.anch_samples > args.anchor_samples_threshold) &
        (df.anchSample_cts > args.anchor_sample_counts_threshold)
    ]

    #### would like to do this, but table gets too large, so just iterating
    # print('pivoting')
    # ctsDf = (df
    #     .pivot(index=['anchor', 'target'], columns='sample', values='counts')
    #     .reset_index()
    #     .fillna(0))
    return df

### split contingency table into train and test data
def splitCounts(mat,downSampleFrac = .5): #slight modification of https://stackoverflow.com/questions/11818215/subsample-a-matrix-python
    keys, counts = zip(*[
    ((i,j), mat[i,j])
        for i in range(mat.shape[0])
        for j in range(mat.shape[1])
        if mat[i,j] > 0
    ])
    # Make the cumulative counts array
    counts = np.array(counts, dtype=np.int64)
    sum_counts = np.cumsum(counts)

    # Decide how many counts to include in the sample
    frac_select = downSampleFrac
    count_select = int(sum_counts[-1] * frac_select)

    # Choose unique counts
    ind_select = sorted(np.random.choice(range(sum_counts[-1]), count_select,replace=False))

    # A vector to hold the new counts
    out_counts = np.zeros(counts.shape, dtype=np.int64)

    # Perform basically the merge step of merge-sort, finding where
    # the counts land in the cumulative array
    i = 0
    j = 0
    while i<len(sum_counts) and j<len(ind_select):
        if ind_select[j] < sum_counts[i]:
            j += 1
            out_counts[i] += 1
        else:
            i += 1

    # Rebuild the matrix using the `keys` list from before
    out_mat = np.zeros(mat.shape, dtype=np.int64)
    for i in range(len(out_counts)):
        out_mat[keys[i]] = out_counts[i]
        
    return out_mat

### simple wrapper of the above to downsample matrix columnwise, as opposed to overall
def splitCountsColwise(mat,downSampleFrac = .5): #slight modification of https://stackoverflow.com/questions/11818215/subsample-a-matrix-python
    out_mat = np.zeros_like(mat)
    for j in range(mat.shape[1]):
        if mat[:,j].sum()>0:
            out_mat[:,j] = splitCounts(np.reshape(mat[:,j],(mat.shape[0],-1)),downSampleFrac).flatten()
        
    return out_mat

### shift and scale an input vector to be in the range [minval, maxval]
def normalizevec(x,minval=0,maxval=1):
    if x.max()==x.min():
        return np.zeros_like(x)
    x = np.array(x)
    x01= (x-x.min())/(x.max()-x.min())
    return x01*(maxval-minval)+minval


#### get spectral c,f, from simple SVD (correspondence analysis style)
def get_spectral_cf_svd(X,tblShape):

    relevantTargs = X.sum(axis=1)>0
    relevantSamples = X.sum(axis=0)>0
    
    r,c=X.shape

    if relevantTargs.sum()<2 or relevantSamples.sum()<2:
        return np.zeros(c),np.zeros(r)

    X = X[np.ix_(relevantTargs,relevantSamples)]

    E = np.outer(X.sum(axis=1),X.sum(axis=0))/X.sum()
    svdmat = (X-E)@np.diag(1.0/X.sum(axis=0))
    u,s,vh=np.linalg.svd(svdmat,full_matrices=False) ### very important to set full_matrices to false, else memory errors

    cRaw = vh[0,:]
    fRaw = u[:,0]

    cGuess = cRaw
    fGuess = normalizevec(fRaw,0,1)


    fElong = .5*np.ones(tblShape[0])
    fElong[relevantTargs] = fGuess
    fOpt = fElong
    
    cElong = np.zeros(tblShape[1])
    cElong[np.arange(tblShape[1])[relevantSamples]]=cGuess ### fancy indexing
    cOpt = cElong
    
    return cOpt,fOpt


### find locally-optimal (unconstrained) c and f from spectral c initialization
def generateSpectralOptcf(X,tblShape):
    np.random.seed(0)### for filling in f
    
    relevantTargs = X.sum(axis=1)>0
    relevantSamples = X.sum(axis=0)>0

    r,c=X.shape
    
    if relevantTargs.sum()<2 or relevantSamples.sum()<2:
        return np.zeros(c),np.zeros(r),0

    X = X[np.ix_(relevantTargs,relevantSamples)]

    
    ###### Spectral initialization of c
    ### pairwise similarity matrix
    A = X.T @ X
    A = A-np.diag(np.diag(A))
    
    ### remove isolated elements
    sampleMask2= (A.sum(axis=0)>0)
    
    if sampleMask2.sum()==0: ### graph is not connected (I think)
        sampleMask2 = np.ones(X.shape[1],dtype='bool')
        c = np.random.choice([-1,1],size=X.shape[1])
    else:
        A = A[np.ix_(sampleMask2,sampleMask2)]
        X = X[:,sampleMask2]

        ## construct diagonals for Laplacian
        D = np.diag(A.sum(axis=0))

        ### spectrally normalized Laplacian
        Dnorm = np.diag(A.sum(axis=0)**(-1/2))
        L = np.eye(A.shape[0]) - Dnorm@A@Dnorm

        ### normal
        # L=D-A

        ### potentially could merge results from 2 clusterings?

        ### compute fiedler vector
        eigvals,eigvecs =np.linalg.eig(L)
        c = np.sign(normalizevec(np.real(eigvecs[:,np.argsort(np.abs(eigvals))[1]])))
        ## maybe more fancy checking needed if graph isn't connected
    
    #### if clustering put all samples in same cluster, shouldn't happen
    if np.all(c==c[0]):
        c[0] = -1*c[0]
    
    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj)) ### avoid divide by 0 errors
    njinvSqrt[nj==0]=0
    
    fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
    fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
    
    Sold = 0
    i=0
    while True:
        
        ### find optimal f for fixed c
        fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
        fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
        fOpt=normalizevec(fOpt)
        
        ### find optimal c for fixed f
        Sj = np.multiply(fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum()),njinvSqrt)

        if np.all(Sj==0): ### something went wrong
            c = np.zeros_like(c)
            fOpt=np.zeros_like(fOpt)
            break
    
        c = Sj/np.linalg.norm(Sj,2)
        
        ### compute objective value, if fixed, stop
        S = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(c*njinvSqrt)/np.linalg.norm(c,2)
        if S==Sold: ### will terminate once fOpt is fixed over 2 iterations
            break
        Sold = S
        i+=1
        if i>50: ### something went wrong
            c = np.zeros_like(c)
            fOpt=np.zeros_like(fOpt)
            break
    
    ## extend to targets and samples that didn't occur in training data
    fElong = np.random.choice([0,1],size=tblShape[0])
    fElong[relevantTargs] = fOpt
    fOpt = fElong
    
    cElong = np.zeros(tblShape[1])
    cElong[np.arange(tblShape[1])[relevantSamples][sampleMask2]]=c ### fancy indexing
    cOpt = cElong
    
    return np.nan_to_num(cOpt,0),np.nan_to_num(normalizevec(fOpt),0),i


### find locally-optimal (unconstrained) c and f from random initialization
def generateRandOptcf(X,tblShape,randSeed=0):
    np.random.seed(randSeed) ### random initialization and extension
    
    relevantTargs = X.sum(axis=1)>0
    relevantSamples = X.sum(axis=0)>0

    r,c=X.shape
    
    if relevantTargs.sum()<2 or relevantSamples.sum()<2:
        return np.zeros(c),np.zeros(r),0

    X = X[np.ix_(relevantTargs,relevantSamples)]
    
    c = np.random.choice([-1,1],size=X.shape[1])
    #### if clustering put all in same cluster, perturb
    
    if np.all(c==c[0]):
        c[0] = -1*c[0]
    
    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj)) ### avoid divide by 0 errors
    njinvSqrt[nj==0]=0
    
    fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
    fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
    
    Sold = 0
    i=0
    while True:
        
        ### find optimal f for fixed c
        fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
        fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
        
        ### find optimal c for fixed f
        Sj = np.multiply(fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum()),njinvSqrt)
        c = Sj/np.linalg.norm(Sj,2)
        
        ### compute objective value, if fixed, stop
        S = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(c*njinvSqrt)/np.linalg.norm(c,2)
        if S==Sold: ### will terminate once fOpt is fixed over 2 iterations
            break
        Sold = S
        i+=1
        if i>50:
            c = np.zeros_like(c)
            fOpt=np.zeros_like(fOpt)
            break
    
    
    ## extend to targets and samples that didn't occur previously
    fElong = np.random.choice([0,1],size=tblShape[0])
    fElong[relevantTargs] = fOpt
    fOpt = fElong
    
    cElong = np.zeros(tblShape[1])
    cElong[np.arange(tblShape[1])[relevantSamples]]=c ### fancy indexing
    cOpt = cElong
    
    return cOpt,fOpt,i


### find locally-optimal c and f from random initialization
#### constrained to c having the same sign as sheetCj
def generateSignedSheetCjOptcf(X,sheetCj,tblShape):
    np.random.seed(0)### for filling in f
    
    relevantTargs = X.sum(axis=1)>0
    relevantSamples = X.sum(axis=0)>0

    X = X[np.ix_(relevantTargs,relevantSamples)]

    cSign = sheetCj[relevantSamples]
    c = cSign

    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj)) ### avoid divide by 0 errors
    njinvSqrt[nj==0]=0
    
    fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
    fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
    
    Sold = 0
    i=0
    while True:
        
        ### find optimal f for fixed c
        fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
        fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
        
        ### find optimal c for fixed f
        Sj = np.multiply(fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum()),njinvSqrt)
        ### construct plus and minus variants of Sj
        cplus = Sj * ((Sj*cSign) >0)
        cplus /= np.maximum(1,np.linalg.norm(cplus,2))
        c=cplus
        Splus = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(c*njinvSqrt)/np.maximum(1,np.linalg.norm(c,2))
        
        cminus = Sj * ((Sj*cSign) <0)
        cminus /= np.maximum(1,np.linalg.norm(cminus,2))
        c=cminus
        Sminus = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(c*njinvSqrt)/np.maximum(1,np.linalg.norm(c,2))

        if Splus >= -1*Sminus:
            c = cplus
            S = Splus
        else:
            c = cminus
            S = Sminus

        if S==Sold: ### will terminate once fOpt is fixed over 2 iterations
            break
        Sold = S
        i+=1
        if i>50:
            c = np.zeros_like(c)
            fOpt=np.zeros_like(fOpt)
            break
            
    ## extend to targets and samples that didn't occur previously
    fElong = np.random.choice([0,1],size=tblShape[0])
    fElong[relevantTargs] = fOpt
    fOpt = fElong
    
    cElong = np.zeros(tblShape[1])
    cElong[np.arange(tblShape[1])[relevantSamples]]=c ### fancy indexing
    cOpt = cElong
    return cOpt,fOpt,i


### find optimal f for given input sheetCj
def generateSheetCjOptcf(X,sheetCj,tblShape):
    np.random.seed(0) ### for filling in f
    
    relevantTargs = X.sum(axis=1)>0
    X = X[relevantTargs]

    ## set cj as sheetCj
    c = sheetCj

    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj)) ### avoid divide by 0 errors
    njinvSqrt[nj==0]=0
    
    ## compute opt f
    fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
    fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
    
    ## extend to targets and samples that didn't occur previously
    fElong = np.random.choice([0,1],size=tblShape[0])
    fElong[relevantTargs] = fOpt
    fOpt = fElong
    
    cOpt = c ### sheetCj
    return cOpt,fOpt


### asymptotic pvalue
### NOMAD is asymptotically normal with variance upper bounded by totalVar under the null
###   allowing is to provide asymptotically valid p-values
def computeAsympNOMAD(X,cOpt,fOpt):
    if (cOpt==0).all():
        return 1
    
    cOpt = np.nan_to_num(cOpt,0)
    fOpt = np.nan_to_num(fOpt,0)
    if not (fOpt.max()<=1) and (fOpt.min()>=0):
        fOpt = normalizevec(np.nan_to_num(fOpt,0))
    
    ### only requirement for valid p-value
    assert((fOpt.max()<=1) and (fOpt.min()>=0))
    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj))
    njinvSqrt[nj==0]=0
    
    ### compute p value
    S = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(cOpt*njinvSqrt)

    if S<1E-10: ### prevent edge issues
        return 1

    M=X.sum()
    
    muhat = (fOpt@X).sum()/M
    
    varF = (fOpt-muhat)**2 @ X.sum(axis=1)/X.sum()
    totalVar = varF * (np.linalg.norm(cOpt)**2 - (cOpt@np.sqrt(nj))**2/M)
    pval = 2*np.exp(-S**2/(2*totalVar ))
                
    return min(np.nan_to_num(pval,1),1)


### test p-value for fixed c and f on contingency table
def testPval(X,cOpt,fOpt):
    
    cOpt = np.nan_to_num(cOpt,0)
    fOpt = np.nan_to_num(fOpt,0)

    if np.all(cOpt == cOpt[0]) or np.all(fOpt==fOpt[0]):
        return 1

    if not (fOpt.max()<=1) and (fOpt.min()>=0):
        fOpt = normalizevec(fOpt,0,1)
    
    ### only requirement for valid p-value
    assert((fOpt.max()<=1) and (fOpt.min()>=0))
    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj))
    njinvSqrt[nj==0]=0
    
    ### compute test statistic
    S = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(cOpt*njinvSqrt)

    M=X.sum()
    
    denom = (np.linalg.norm(cOpt)**2 - (cOpt@np.sqrt(nj))**2/M)
    pval = 2*np.exp(-2*S**2/denom)
                
    # if np.isnan(pval):
    #     print(S,denom,cOpt, fOpt@X/ nj)

    return min(np.nan_to_num(pval,1),1)

### standard effect size definition, for +/-1 (or, binarized) c
def effectSize_bin(X,c,f):
    if (c>0).sum()==0 or (c<0).sum()==0:
        return 0

    return np.abs(f@X@(c>0) / (X@(c>0)).sum() - f@X@(c<0) / (X@(c<0)).sum())


### new effect size definition, for continuous c
def effectSize_cts(X,c,f):
    if (X@np.abs(c)).sum() ==0:
        return 0

    return np.abs(f@X@c / (X@np.abs(c)).sum())


### compute chi2 pvalue
def computeChi2Test(X):
    if len(X.shape)==1:
        return 1
    X = X[X.sum(axis=1)>0]
    X = X[:,X.sum(axis=0)>0]
    _,pv,_,_= scipy.stats.contingency.chi2_contingency(X)
    return pv

### compute LRT against full null
def computeLRT_Test(X):
    if len(X.shape)==1:
        return 1
    X = X[X.sum(axis=1)>0]
    X = X[:,X.sum(axis=0)>0]
    _,pv,_,_ = scipy.stats.contingency.chi2_contingency(X, lambda_="log-likelihood")
    return pv



def printhw():
    print('hello world')


