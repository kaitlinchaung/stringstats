import numpy as np
import pandas as pd
from tqdm import tqdm,tqdm_notebook, tqdm_pandas
import os
import glob
import pickle
import argparse
import scipy
import scipy.stats
import statsmodels.api as sm

##### work in progress to appear in upcoming submission


# README.md
# Two knobs which have not been made programatically settable are *trainFrac* and *numRandNOMAD*.
# The former indicates what fraction of data is to be randomly split for the training portion, and the second indicates how many random *c,f* are to be simulated for the traditional NOMAD p-value.
# These are currently hardcoded as .2 and 20 respectively, where *numRandNOMAD* significantly impacts the overall runtime (setting it to 100 vs 0 increases overall runtime by a factor of >10), as this is not tensor-optimized as in compute_pvals.py.

# ## Columns
# Many different p-values are generated by NOMAD and output to spectral_pvals.tsv. The corresponding BY-corrected q-values are included in spectral_pvals_corrected.tsv.
# We provide 4 variants of p-values which split off a train dataset, and optimize the p-value objective on this held out dataset:
# **pval_spectral:** train test split, spectral initialization on train, then EM optimizaiton
# **pval_rand_init_EM:** train test split, random initialization, EM optimization
# **pvals_optimized_samplesheet:**  train test split, fixed samplesheet cj, optimized f
# **pvals_optimized_samplesheetSigned:** train test split, samplesheet cj initialization, EM optimization (with cj sign constraint)


# In addition to this, we provide the classical NOMAD p-value, a variant where *f,c* are drawn to be continuous valued, and a variant related to correspondence analysis:
# **nomad_pv:** normal NOMAD p-value, ${numRandNOMAD} random {-1,1} valued c and {0,1} valued f used
# **nomad_cts_pv:** NOMAD p-value, ${numRandNOMAD} random [-1,1] valued c and [0,1] valued f used (uniform)
# **nomad_spectral_corrAnalysis_pv:** train test split, simple correspondence analysis style SVD to compute c and f

# We further provide tighter, asymptotically valid p-values for these quantities:
# **nomad_asymp_pv:** NOMAD asymptotic p-value, ${numRandNOMAD} random {-1,1} valued c and {0,1} valued f used
# **nomad_asymp_cts_pv:** NOMAD asymptotic p-value, ${numRandNOMAD} random [-1,1] valued c and [0,1] valued f used (uniform)
# **nomad_spectral_corrAnalysis_asymp_pv:** train test split, simple correspondence analysis style SVD to compute c and f, asymptotic p-value

# Finally, we also provide 2 generic p-values, which are fundamentally only asymptotically valid:
# **chi2pval:** chi-squared test
# **lrtpval:** likelihood ratio test


## Outputs:
### args.outfldr+'/spectral_pvalues.tsv': tsv of raw p-values
### args.outfldr+'/spectral_pvalues_corrected.tsv': tsv of BY-corrected q-values


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument( ### input abundant_stratified_{}.txt.gz folder 
        "--strat_fldr",
        type=str
    )
    parser.add_argument( ### target output folder, required
        "--outfldr",
        type=str
    )
    parser.add_argument( ### samplesheet file, if samplesheet-based cj (metadata) are to be used
        "--samplesheet",
        type=str,
        default=""
    )
    parser.add_argument( ### optional file indicating subset of anchors to be used
        "--anchor_list",
        type=str,
        default=""
    )
    parser.add_argument( ### flag, whether to save cj or not
        "--save_c_f",
        type=bool,
        default=False
        ### if save_c_f flag, then can read in optimizing c and f as below:
        #### with open(args.outfldr+'/spectral_cj.npy','rb') as f:
        ####     a = np.load(f)
        #### with open(args.outfldr+'/spectral_f.pkl', 'rb') as handle:
        ####     b = pickle.load(handle)
    )
    args = parser.parse_args()
    return args


### read in samplsheet, and output samplesheetCj (properly ordered)
def parseSamplesheet(samplesheet,sampleNames):
    sheetCj = np.ones(len(sampleNames))
    if samplesheet!='':
        with open(samplesheet,'r') as f:
            cols = f.readline().split(',')
        if len(cols)==1: ### if len(cols) is 1, then only samplesheet name, no ids
            print("Only 1 samplesheet column, using random cjs")
        elif len(cols)>2:
            print("Improperly formatted samplesheet")
        else:

            sheetdf = pd.read_csv(samplesheet,names=['fname','sheetCjs'])
            sheetdf['sample'] = (sheetdf.fname
                            .str.rsplit('/',1,expand=True)[1]
                            .str.split('.',1,expand=True)[0])
            sheetdf = sheetdf.drop(columns='fname')
            sheetdf['sheetCjs'] = normalizevec(sheetdf.sheetCjs,-1,1)

            sheetCj = sheetdf.set_index('sample').T[sampleNames].to_numpy().flatten()
            print('using samplesheet metadata: not fully tested, be warned')
    return sheetCj

### construct contingency tables, only for anchors in anchLst
def constructCountsDf(strat_fldr,anchLst):
    kmer_size = 27
    dfs = []
    paths = glob.glob(strat_fldr+"/abundant_stratified_*.txt.gz")
    print('reading in abudant_stratified files')
    for path in tqdm(paths,total=len(paths)):
        df = pd.read_csv(path.strip(), delim_whitespace=True, names=['counts','seq','sample'])
        df['anchor'] = df.seq.str[:kmer_size]
        df['target'] = df.seq.str[kmer_size:]
        df = df.drop(columns='seq').drop_duplicates()
        if anchLst != []:
            df = df[df.anchor.isin(anchLst)]
        dfs.append(df)

    print('concat and pivoting')
    ctsDf = pd.concat(dfs)
    ctsDf = (ctsDf
        .pivot(index=['anchor', 'target'], columns='sample', values='counts')
        .reset_index()
        .fillna(0))
    return ctsDf

### split contingency table into train and test data
def splitCounts(mat,downSampleFrac = .5): #slight modification of https://stackoverflow.com/questions/11818215/subsample-a-matrix-python
    keys, counts = zip(*[
    ((i,j), mat[i,j])
        for i in range(mat.shape[0])
        for j in range(mat.shape[1])
        if mat[i,j] > 0
    ])
    # Make the cumulative counts array
    counts = np.array(counts, dtype=np.int64)
    sum_counts = np.cumsum(counts)

    # Decide how many counts to include in the sample
    frac_select = downSampleFrac
    count_select = int(sum_counts[-1] * frac_select)

    # Choose unique counts
    ind_select = sorted(np.random.choice(range(sum_counts[-1]), count_select,replace=False))

    # A vector to hold the new counts
    out_counts = np.zeros(counts.shape, dtype=np.int64)

    # Perform basically the merge step of merge-sort, finding where
    # the counts land in the cumulative array
    i = 0
    j = 0
    while i<len(sum_counts) and j<len(ind_select):
        if ind_select[j] < sum_counts[i]:
            j += 1
            out_counts[i] += 1
        else:
            i += 1

    # Rebuild the matrix using the `keys` list from before
    out_mat = np.zeros(mat.shape, dtype=np.int64)
    for i in range(len(out_counts)):
        out_mat[keys[i]] = out_counts[i]
        
    return out_mat

### simple wrapper of the above to downsample matrix columnwise, as opposed to overall
def splitCountsColwise(mat,downSampleFrac = .5): #slight modification of https://stackoverflow.com/questions/11818215/subsample-a-matrix-python
    out_mat = np.zeros_like(mat)
    for j in range(mat.shape[1]):
        if mat[:,j].sum()>0:
            out_mat[:,j] = splitCounts(np.reshape(mat[:,j],(mat.shape[0],-1)),downSampleFrac).flatten()
        
    return out_mat

### shift and scale an input vector to be in the range [minval, maxval]
def normalizevec(x,minval=0,maxval=1):
    if x.max()==x.min():
        return np.zeros_like(x)
    x = np.array(x)
    x01= (x-x.min())/(x.max()-x.min())
    return x01*(maxval-minval)+minval


#### get spectral c,f, from simple SVD (correspondence analysis style)
def get_spectral_cf_svd(X,tblShape):

    relevantTargs = X.sum(axis=1)>0
    relevantSamples = X.sum(axis=0)>0
    
    if relevantTargs.sum()<2 or relevantSamples.sum()<2:
        print('error')
        return

    X = X[np.ix_(relevantTargs,relevantSamples)]

    E = np.outer(X.sum(axis=1),X.sum(axis=0))/X.sum()
    svdmat = (X-E)@np.diag(1.0/X.sum(axis=0))
    u,s,vh=np.linalg.svd(svdmat,full_matrices=False) ### very important to set full_matrices to false, else memory errors

    cRaw = vh[0,:]
    fRaw = u[:,0]

    cGuess = cRaw
    fGuess = normalizevec(fRaw,0,1)


    fElong = .5*np.ones(tblShape[0])
    fElong[relevantTargs] = fGuess
    fOpt = fElong
    
    cElong = np.zeros(tblShape[1])
    cElong[np.arange(tblShape[1])[relevantSamples]]=cGuess ### fancy indexing
    cOpt = cElong
    
    return cOpt,fOpt


### find locally-optimal (unconstrained) c and f from spectral c initialization
def generateSpectralOptcf(X,tblShape):
    np.random.seed(0)### for filling in f
    
    relevantTargs = X.sum(axis=1)>0
    relevantSamples = X.sum(axis=0)>0
    
    if relevantTargs.sum()<2 or relevantSamples.sum()<2:
        print('too few row/col for generateSpectralOptcf')
        return

    X = X[np.ix_(relevantTargs,relevantSamples)]

    
    ###### Spectral initialization of c
    ### pairwise similarity matrix
    A = X.T @ X
    A = A-np.diag(np.diag(A))
    
    ### remove isolated elements
    sampleMask2= (A.sum(axis=0)>0)
    
    if sampleMask2.sum()==0: ### graph is not connected (I think)
        sampleMask2 = np.ones(X.shape[1],dtype='bool')
        c = np.random.choice([-1,1],size=X.shape[1])
    else:
        A = A[np.ix_(sampleMask2,sampleMask2)]
        X = X[:,sampleMask2]

        ## construct diagonals for Laplacian
        D = np.diag(A.sum(axis=0))

        ### spectrally normalized Laplacian
        Dnorm = np.diag(A.sum(axis=0)**(-1/2))
        L = np.eye(A.shape[0]) - Dnorm@A@Dnorm

        ### normal
        # L=D-A

        ### potentially could merge results from 2 clusterings?

        ### compute fiedler vector
        eigvals,eigvecs =np.linalg.eig(L)
        c = np.sign(normalizevec(np.real(eigvecs[:,np.argsort(np.abs(eigvals))[1]])))
        ## maybe more fancy checking needed if graph isn't connected
    
    #### if clustering put all samples in same cluster, shouldn't happen
    if np.all(c==c[0]):
        c[0] = -1*c[0]
    
    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj)) ### avoid divide by 0 errors
    njinvSqrt[nj==0]=0
    
    fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
    fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
    
    Sold = 0
    i=0
    while True:
        
        ### find optimal f for fixed c
        fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
        fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
        fOpt=normalizevec(fOpt)
        
        ### find optimal c for fixed f
        Sj = np.multiply(fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum()),njinvSqrt)

        if np.all(Sj==0): ### something went wrong
            c = np.zeros_like(c)
            fOpt=np.zeros_like(fOpt)
            break
    
        c = Sj/np.linalg.norm(Sj,2)
        
        ### compute objective value, if fixed, stop
        S = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(c*njinvSqrt)/np.linalg.norm(c,2)
        if S==Sold: ### will terminate once fOpt is fixed over 2 iterations
            break
        Sold = S
        i+=1
        if i>50: ### something went wrong
            c = np.zeros_like(c)
            fOpt=np.zeros_like(fOpt)
            break
    
    ## extend to targets and samples that didn't occur in training data
    fElong = np.random.choice([0,1],size=tblShape[0])
    fElong[relevantTargs] = fOpt
    fOpt = fElong
    
    cElong = np.zeros(tblShape[1])
    cElong[np.arange(tblShape[1])[relevantSamples][sampleMask2]]=c ### fancy indexing
    cOpt = cElong
    
    return np.nan_to_num(cOpt,0),np.nan_to_num(normalizevec(fOpt),0),i


### find locally-optimal (unconstrained) c and f from random initialization
def generateRandOptcf(X,tblShape,randSeed=0):
    np.random.seed(randSeed) ### random initialization and extension
    
    relevantTargs = X.sum(axis=1)>0
    relevantSamples = X.sum(axis=0)>0
    
    if relevantTargs.sum()<2 or relevantSamples.sum()<2:
        print('error')
        return

    X = X[np.ix_(relevantTargs,relevantSamples)]
    
    c = np.random.choice([-1,1],size=X.shape[1])
    #### if clustering put all in same cluster, perturb
    
    if np.all(c==c[0]):
        c[0] = -1*c[0]
    
    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj)) ### avoid divide by 0 errors
    njinvSqrt[nj==0]=0
    
    fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
    fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
    
    Sold = 0
    i=0
    while True:
        
        ### find optimal f for fixed c
        fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
        fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
        
        ### find optimal c for fixed f
        Sj = np.multiply(fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum()),njinvSqrt)
        c = Sj/np.linalg.norm(Sj,2)
        
        ### compute objective value, if fixed, stop
        S = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(c*njinvSqrt)/np.linalg.norm(c,2)
        if S==Sold: ### will terminate once fOpt is fixed over 2 iterations
            break
        Sold = S
        i+=1
        if i>50:
            c = np.zeros_like(c)
            fOpt=np.zeros_like(fOpt)
            break
    
    
    ## extend to targets and samples that didn't occur previously
    fElong = np.random.choice([0,1],size=tblShape[0])
    fElong[relevantTargs] = fOpt
    fOpt = fElong
    
    cElong = np.zeros(tblShape[1])
    cElong[np.arange(tblShape[1])[relevantSamples]]=c ### fancy indexing
    cOpt = cElong
    
    return cOpt,fOpt,i


### find locally-optimal c and f from random initialization
#### constrained to c having the same sign as sheetCj
def generateSignedSheetCjOptcf(X,sheetCj,tblShape):
    np.random.seed(0)### for filling in f
    
    relevantTargs = X.sum(axis=1)>0
    relevantSamples = X.sum(axis=0)>0

    X = X[np.ix_(relevantTargs,relevantSamples)]

    cSign = sheetCj[relevantSamples]
    c = cSign

    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj)) ### avoid divide by 0 errors
    njinvSqrt[nj==0]=0
    
    fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
    fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
    
    Sold = 0
    i=0
    while True:
        
        ### find optimal f for fixed c
        fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
        fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
        
        ### find optimal c for fixed f
        Sj = np.multiply(fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum()),njinvSqrt)
        ### construct plus and minus variants of Sj
        cplus = Sj * ((Sj*cSign) >0)
        cplus /= np.maximum(1,np.linalg.norm(cplus,2))
        c=cplus
        Splus = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(c*njinvSqrt)/np.maximum(1,np.linalg.norm(c,2))
        
        cminus = Sj * ((Sj*cSign) <0)
        cminus /= np.maximum(1,np.linalg.norm(cminus,2))
        c=cminus
        Sminus = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(c*njinvSqrt)/np.maximum(1,np.linalg.norm(c,2))

        if Splus >= -1*Sminus:
            c = cplus
            S = Splus
        else:
            c = cminus
            S = Sminus

        if S==Sold: ### will terminate once fOpt is fixed over 2 iterations
            break
        Sold = S
        i+=1
        if i>50:
            c = np.zeros_like(c)
            fOpt=np.zeros_like(fOpt)
            break
            
    ## extend to targets and samples that didn't occur previously
    fElong = np.random.choice([0,1],size=tblShape[0])
    fElong[relevantTargs] = fOpt
    fOpt = fElong
    
    cElong = np.zeros(tblShape[1])
    cElong[np.arange(tblShape[1])[relevantSamples]]=c ### fancy indexing
    cOpt = cElong
    return cOpt,fOpt,i


### find optimal f for given input sheetCj
def generateSheetCjOptcf(X,sheetCj,tblShape):
    np.random.seed(0) ### for filling in f
    
    relevantTargs = X.sum(axis=1)>0
    X = X[relevantTargs]

    ## set cj as sheetCj
    c = sheetCj

    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj)) ### avoid divide by 0 errors
    njinvSqrt[nj==0]=0
    
    ## compute opt f
    fOpt = np.sign((X-X@np.outer(np.ones(X.shape[1]),nj)/np.maximum(1,X.sum()))@(c*njinvSqrt))
    fOpt = (fOpt+1)/2 ### to rescale f to be [0,1] valued
    
    ## extend to targets and samples that didn't occur previously
    fElong = np.random.choice([0,1],size=tblShape[0])
    fElong[relevantTargs] = fOpt
    fOpt = fElong
    
    cOpt = c ### sheetCj
    return cOpt,fOpt


### test p-value for fixed c and f on contingency table
def testPval(X,cOpt,fOpt):
    if (cOpt==0).all():
        return 1
    
    cOpt = np.nan_to_num(cOpt,0)
    fOpt = np.nan_to_num(fOpt,0)
    if not (fOpt.max()<=1) and (fOpt.min()>=0):
        fOpt = normalizevec(np.nan_to_num(fOpt,0))
    
    ### only requirement for valid p-value
    assert((fOpt.max()<=1) and (fOpt.min()>=0))
    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj))
    njinvSqrt[nj==0]=0
    
    ### compute p value
    S = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(cOpt*njinvSqrt)
    M=X.sum()
    num = (cOpt**2).sum()*M 
    denom=(cOpt@np.sqrt(nj))**2
    with np.errstate(divide='ignore', invalid='ignore'):
        if num==0:
            a= 1
        elif denom==0:
            a=0
        else:
            a = 1.0/(np.sqrt(num/denom)+1.0)
    
        term1HC = 2*np.exp(- 2*(1-a)**2*S**2
                       /(cOpt**2).sum())
        term2HC = 2*np.exp(-2*a**2*M*S**2
                           /(cOpt@np.sqrt(nj))**2 )

    if a==0:
        pv = term1HC
    else:
        pv = term1HC+term2HC
        
    return min(1,pv)


### asymptotic pvalue
### NOMAD is asymptotically normal with variance upper bounded by totalVar under the null
###   allowing is to provide asymptotically valid p-values
def computeAsympNOMAD(X,cOpt,fOpt):
    if (cOpt==0).all():
        return 1
    
    cOpt = np.nan_to_num(cOpt,0)
    fOpt = np.nan_to_num(fOpt,0)
    if not (fOpt.max()<=1) and (fOpt.min()>=0):
        fOpt = normalizevec(np.nan_to_num(fOpt,0))
    
    ### only requirement for valid p-value
    assert((fOpt.max()<=1) and (fOpt.min()>=0))
    nj = X.sum(axis=0)
    njinvSqrt = 1.0/np.maximum(1,np.sqrt(nj))
    njinvSqrt[nj==0]=0
    
    ### compute p value
    S = fOpt @ (X-X@np.outer(np.ones(X.shape[1]),nj)/X.sum())@(cOpt*njinvSqrt)

    if S<1E-10: ### prevent edge issues
        return 1

    M=X.sum()
    
    muhat = (fOpt@X).sum()/M
    
    varF = (fOpt-muhat)**2 @ X.sum(axis=1)/X.sum()
    totalVar = varF * (np.linalg.norm(cOpt)**2 - (cOpt@np.sqrt(nj))**2/M)
    pval = 2*np.exp(-S**2/(2*totalVar ))
                
    return min(np.nan_to_num(pval,1),1)


def effectSize(X,c,f):
    ### binary effect size
    if (c>0).sum()==0 or (c<0).sum()==0:
        effectBin = 0
    else:
        effectBin = np.abs(f@X@(c>0) / (X@(c>0)).sum() - f@X@(c<0) / (X@(c<0)).sum())


    ### new effect size definition
    if (X@np.abs(c)).sum() ==0:
        effectRaw=0
    else:
        effectRaw = np.abs(f@X@c / (X@np.abs(c)).sum())

    return effectBin,effectRaw


### compute chi2 pvalue
def computeChi2Test(X):
    if len(X.shape)==1:
        return 1
    X = X[X.sum(axis=1)>0]
    X = X[:,X.sum(axis=0)>0]
    _,pv,_,_= scipy.stats.contingency.chi2_contingency(X)
    return pv

### compute LRT against full null
def computeLRT_Test(X):
    if len(X.shape)==1:
        return 1
    X = X[X.sum(axis=1)>0]
    X = X[:,X.sum(axis=0)>0]
    _,pv,_,_ = scipy.stats.contingency.chi2_contingency(X, lambda_="log-likelihood")
    return pv





#### main function
def main():
    args = get_args()
    print('running main with save =', args.save_c_f)

    if not os.path.exists(args.outfldr):
        os.makedirs(args.outfldr)

    ### read in anchor list file
    if args.anchor_list!='':
        print("using passed in anchor list")
        anchLst = pd.read_csv(args.anchor_list,names=['anchor']).anchor.to_list()
        if len(anchLst)==0:
            print('no anchors in list')
            return
        print(len(anchLst), "anchors")
    else:
        print('using all anchors')
        anchLst = []

    print('constructing counts dataframe')
    ctsDf = constructCountsDf(args.strat_fldr,anchLst)
    print('done with counts dataframe')

    if anchLst == []:
        anchLst = ctsDf.reset_index().anchor.unique()
        print('generated all anchors, ', len(anchLst))

    sampleNames = ctsDf.columns[2:].to_list()
    sheetCj = parseSamplesheet(args.samplesheet,sampleNames)
    useSheetCj = not np.all(sheetCj==sheetCj[0])
    ctsDf = ctsDf.loc[:,['anchor','target']+sampleNames].set_index('anchor')
    
    #### some ad-hoc choices: can be made command line arguments eventually
    readMinimum = 10
    trainFrac=.2
    numRandNOMAD = 1


    #### all of the arrays to fill
    pvalsSpectral = np.ones(len(anchLst)) ### train test split, spectral initialization on train, then EM optimizaiton
    pvalsRandOpt = np.ones(len(anchLst)) ### train test split, random initialization, EM optimization
    pvalsSheet=np.ones(len(anchLst)) ### train test split, fixed samplesheet cj, optimized f
    pvalsSheetSign=np.ones(len(anchLst)) ### train test split, samplesheet cj initialization, EM optimization (with cj sign constraint)
    chi2Arr = np.ones(len(anchLst)) ### chi-squared test
    lrtArr = np.ones(len(anchLst)) ### likelihood ratio test
    nomad_simpleSVD_pv = np.ones(len(anchLst)) ### train test split, simple correspondence analysis style SVD to compute c and f
    nomad_asympSVD_pv = np.ones(len(anchLst)) ### train test split, simple correspondence analysis style SVD to compute c and f, asymptotic p-value
    nomad_normalPV = np.ones(len(anchLst)) ### normal NOMAD p-value, ${numRandNOMAD} random {-1,1} valued c and {0,1} valued f used
    nomadAsympPV = np.ones(len(anchLst)) ### NOMAD asymptotic p-value, ${numRandNOMAD} random {-1,1} valued c and {0,1} valued f used
    nomad_cts_pv = np.ones_like(chi2Arr) ### NOMAD p-value, ${numRandNOMAD} random [-1,1] valued c and [0,1] valued f used (uniform)
    nomad_asymp_rand_cts = np.ones_like(chi2Arr) ### NOMAD asymptotic p-value, ${numRandNOMAD} random [-1,1] valued c and [0,1] valued f used (uniform)



    eSizeArr = np.zeros((len(anchLst),8))
    iArr = np.zeros(len(anchLst))
    if args.save_c_f:
        cjArr = np.zeros((len(anchLst),4,len(sampleNames)))
        fArr = [[] for _ in range(4)] ### matrices are different sizes

    anchsUsed = np.ones(len(anchLst),dtype='bool')
    for anch_idx,anch in tqdm(enumerate(anchLst),total=len(anchLst)):
        if anch not in ctsDf.index:
            print('anchor not present', anch)
            anchsUsed[anch_idx]=False
            continue

        ### load in data matrix (counts matrix)
        ctsLoad = ctsDf.loc[anch,sampleNames].to_numpy()
        
        if ctsLoad.sum()<readMinimum or len(ctsLoad.shape)==1 or ctsLoad.shape[0] <=1 or ctsLoad.shape[1]<=1:
            continue

        ### compute asymptotically valid comparison tests
        chi2Arr[anch_idx] = computeChi2Test(ctsLoad)
        lrtArr[anch_idx] = computeLRT_Test(ctsLoad)

        np.random.seed(0) #### to make it deterministic 

        X = splitCountsColwise(ctsLoad,trainFrac)
        Xtrain = X
        Xtest = ctsLoad-Xtrain
        
        relevantTargs = Xtrain.sum(axis=1)>0
        relevantSamples = Xtrain.sum(axis=0)>0
        
        if relevantTargs.sum()<2 or relevantSamples.sum()<2: ### can't compute anything valuable on this table, declare p-value bound of 1 and continue
            continue
        
        ### compute pvalsSpectral
        cOpt,fOpt,num_iters = generateSpectralOptcf(Xtrain,ctsLoad.shape)
        pvalsSpectral[anch_idx]=testPval(Xtest,cOpt,fOpt)
        eSizeArr[anch_idx,[0,1]]=effectSize(ctsLoad,cOpt,fOpt)
        if args.save_c_f:
            cjArr[anch_idx,0]=cOpt
            fArr[0].append(fOpt)
        
        ### compute pvalsRandOpt
        cOpt,fOpt,num_iters = generateRandOptcf(Xtrain,ctsLoad.shape)
        pvalsRandOpt[anch_idx]=testPval(Xtest,cOpt,fOpt)
        eSizeArr[anch_idx,[2,3]]=effectSize(ctsLoad,cOpt,fOpt)
        if args.save_c_f:
            cjArr[anch_idx,1]=cOpt
            fArr[1].append(fOpt)        
        
        ### compute simple c,f from spectral approach (correspondence analysis style)
        ###   and compute nomad_simpleSVD_pv
        cOpt,fOpt = get_spectral_cf_svd(Xtrain,ctsLoad.shape)
        nomad_simpleSVD_pv[anch_idx] = testPval(Xtest,cOpt,fOpt)
        nomad_asympSVD_pv[anch_idx] = computeAsympNOMAD(Xtest,cOpt,fOpt)

        ### compute nomad_normalPV
        nomadpvminarr = np.zeros(numRandNOMAD)
        nomadasympArr = np.zeros(numRandNOMAD)
        randCs = np.random.choice([-1,1],size=(numRandNOMAD,len(cOpt)))
        randFs = np.random.choice([0,1],size=(numRandNOMAD, len(fOpt)))
        for k in range(numRandNOMAD):
            nomadpvminarr[k] = testPval(ctsLoad,randCs[k], randFs[k])
            nomadasympArr[k] = computeAsympNOMAD(Xtest,cOpt,fOpt)
        nomad_normalPV[anch_idx] = min(1,numRandNOMAD*nomadpvminarr.min())
        nomadAsympPV[anch_idx] = min(1,numRandNOMAD*nomadasympArr.min())

        ### compute for continuous c,f: nomad_cts_pv, nomad_asymp_rand_cts
        nomadasympArr = np.zeros(numRandNOMAD)
        nomadctsArr = np.zeros(numRandNOMAD)
        randCs = np.random.uniform(low=-1,high=1,size=(numRandNOMAD,len(cOpt)))
        randFs = np.random.uniform(size=(numRandNOMAD, len(fOpt)))
        for k in range(numRandNOMAD):
            nomadctsArr[k] = testPval(ctsLoad,randCs[k], randFs[k])
            nomadasympArr[k] = computeAsympNOMAD(ctsLoad,randCs[k],randFs[k])
        nomad_asymp_rand_cts[anch_idx] = min(1,numRandNOMAD*nomadasympArr.min())
        nomad_cts_pv[anch_idx] = min(1,numRandNOMAD*nomadctsArr.min())


        if useSheetCj: ### not fully tested, use with caution
            cOpt,fOpt,num_iters = generateSignedSheetCjOptcf(Xtrain,sheetCj,ctsLoad.shape)
            pvalsSheetSign[anch_idx]=testPval(Xtest,cOpt,fOpt)
            eSizeArr[anch_idx,[4,5]]=effectSize(ctsLoad,cOpt,fOpt)
            if args.save_c_f:
                cjArr[anch_idx,2]=cOpt
                fArr[2].append(fOpt)
            
            cOpt,fOpt = generateSheetCjOptcf(Xtrain,sheetCj,ctsLoad.shape)
            pvalsSheet[anch_idx]=testPval(Xtest,cOpt,fOpt)
            eSizeArr[anch_idx,[6,7]]=effectSize(ctsLoad,cOpt,fOpt)
            if args.save_c_f:
                cjArr[anch_idx,3]=cOpt
                fArr[3].append(fOpt)

    # outdf = pd.DataFrame({'anchor':anchLst, 'pval_spectral':pvalsSpectral, 'pval_rand_init_EM':pvalsRandOpt,
    # 'pvals_optimized_samplesheetSigned':pvalsSheetSign, 'pvals_optimized_samplesheet':pvalsSheet})


    outdf = pd.DataFrame({'anchor':anchLst})

    outdf['pval_spectral'] = pvalsSpectral
    outdf['pval_rand_init_EM'] = pvalsRandOpt

    outdf['chi2pval'] = chi2Arr
    outdf['lrtpval']=lrtArr
    outdf['nomad_spectral_corrAnalysis_pv'] = nomad_simpleSVD_pv
    outdf['nomad_spectral_corrAnalysis_asymp_pv'] = nomad_asympSVD_pv
    outdf['nomad_pv']=nomad_normalPV
    outdf['nomad_asymp_pv']=nomadAsympPV
    outdf['nomad_asymp_cts_pv'] = nomad_asymp_rand_cts
    outdf['nomad_cts_pv'] = nomad_cts_pv

    ### add in effect size measures
    outdf['effect_size_binary_spectral'] = eSizeArr[:,0]
    outdf['effect_size_spectral'] =eSizeArr[:,1]
    outdf['effect_size_binary_rand_init_EM'] = eSizeArr[:,2]
    outdf['effect_size_rand_init_EM'] = eSizeArr[:,3]
    if useSheetCj:
        outdf['effect_size_binary_optimized_samplesheetSigned']=eSizeArr[:,4]
        outdf['effect_size_optimized_samplesheetSigned']=eSizeArr[:,5]
        outdf['effect_size_binary_optimized_samplesheet']=eSizeArr[:,6]
        outdf['effect_size_optimized_samplesheet']=eSizeArr[:,7]

        ### add in sheetcj-based p-values
        outdf['pvals_optimized_samplesheetSigned'] = pvalsSheetSign
        outdf['pvals_optimized_samplesheet'] = pvalsSheet

    outdf = outdf[anchsUsed.astype('bool')]
    outdf = outdf.sort_values('pval_rand_init_EM')
    outdf.to_csv(args.outfldr+'/spectral_pvalues.tsv', sep='\t', index=False)


    ### create corrected pvalues
    newDF = pd.DataFrame({'anchor':outdf.anchor})
    ### this is hacky and could break
    for c in outdf.columns[outdf.columns.str.contains('pv')]:
        _, c_corrected, _, _ = sm.stats.multipletests(outdf[c], method='fdr_by')
        newDF[c+'_corrected'] = c_corrected
        
    ### merge in other columns (effect size)
    newDF.merge(outdf[outdf.columns[~outdf.columns.str.contains('pv')]])
    newDF.to_csv(args.outfldr+'/spectral_pvalues_corrected.tsv', sep='\t', index=False)

    if args.save_c_f:
        if not useSheetCj:
            cjArr = cjArr[:,:4]
        cjArr = cjArr[anchsUsed]
        with open(args.outfldr+'/spectral_cj.npy', 'wb') as f:
            np.save(f,cjArr)

        with open(args.outfldr+'/spectral_f.pkl', 'wb') as handle:
            pickle.dump(fArr, handle, protocol=pickle.HIGHEST_PROTOCOL)

        #### to be read in as below
        # with open(args.outfldr+'/spectral_cj.npy','rb') as f:
        #     a = np.load(f)
        # with open(args.outfldr+'/spectral_f.pkl', 'rb') as handle:
        #     b = pickle.load(handle)


print('starting spectral p value computation')
main()